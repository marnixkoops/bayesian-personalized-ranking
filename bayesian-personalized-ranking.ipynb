{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Personalized Ranking (BPR)\n",
    "> Item recommendation is the task of predicting a personalized ranking on a set of items (e.g. websites, movies, products).\n",
    "We exploit user browsing data (clicks) as a source implicit feedback. There are many methods for item recommendation from implicit feedback like matrix factorization (MF) or adaptive knearest-neighbor (kNN).\n",
    "Even though these methods are designed for the item prediction task of personalized ranking, none of them is directly optimized for ranking.\n",
    "The paper introduces a generic optimization criterion BPR-Opt for personalized ranking that is the maximum posterior estimator derived from a Bayesian analysis of the problem.\n",
    "The learning method is based on stochastic gradient descent with bootstrap sampling.\n",
    "\n",
    "Implementation of [this research paper](https://arxiv.org/abs/1205.2618).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  IMPORTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import time\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  EXPERIMENT SETTINGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running TensorFlow version 1.15.0 on CPU\n"
     ]
    }
   ],
   "source": [
    "all_devices = str(device_lib.list_local_devices())\n",
    "gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if \"GPU\" in all_devices:\n",
    "    DEVICE = \"GPU\"\n",
    "    MACHINE = \"Cloud VM\"\n",
    "elif \"CPU\" in all_devices:\n",
    "    DEVICE = \"CPU\"\n",
    "    MACHINE = \"Local Machine\"\n",
    "\n",
    "print(\" Running TensorFlow version {} on {}\".format(tf.__version__, DEVICE))\n",
    "\n",
    "# model constants\n",
    "epochs = 48\n",
    "batches = 64\n",
    "n_latent_vars = 64  # number of latent features in the MF\n",
    "n_triplets = 5000  # how many (u,i,j) triplets we sample for each batch\n",
    "\n",
    "# lambda regularization strength\n",
    "lambda_id = 0.001\n",
    "lambda_item = 0.001\n",
    "lambda_bias = 0.001\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  PREPARE DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>visit_date</th>\n",
       "      <th>product_views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73ce8f218aea5d95b3a29264446da0aa</td>\n",
       "      <td>20191014</td>\n",
       "      <td>774942,172298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4b9b92eb31de18fbd5a3d6c4e27dabca</td>\n",
       "      <td>20191014</td>\n",
       "      <td>832354,184434,184388,184434,345944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1561782ee52b1c7e5a5b2a669f39d357</td>\n",
       "      <td>20191014</td>\n",
       "      <td>818807,818807,818807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  visit_date  \\\n",
       "0  73ce8f218aea5d95b3a29264446da0aa    20191014   \n",
       "1  4b9b92eb31de18fbd5a3d6c4e27dabca    20191014   \n",
       "2  1561782ee52b1c7e5a5b2a669f39d357    20191014   \n",
       "\n",
       "                        product_views  \n",
       "0                       774942,172298  \n",
       "1  832354,184434,184388,184434,345944  \n",
       "2                818807,818807,818807  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_prep = time.time()  # start timer for preparing data\n",
    "\n",
    "# load input data\n",
    "df = pd.read_csv(\"./data/product_views.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>clicks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000013e374882b95ec0aaf50c8bbfc5f</td>\n",
       "      <td>188643</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000013e374882b95ec0aaf50c8bbfc5f</td>\n",
       "      <td>188710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000021ac02ec5494a4985bc4ec9bdf1c</td>\n",
       "      <td>831957</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id product_id  clicks\n",
       "0  000013e374882b95ec0aaf50c8bbfc5f     188643       1\n",
       "1  000013e374882b95ec0aaf50c8bbfc5f     188710       1\n",
       "2  000021ac02ec5494a4985bc4ec9bdf1c     831957       1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_map_df = pd.read_csv(\"./data/product_mapping.csv\")  # product id to name key-values\n",
    "product_map_df[\"product_id\"] = product_map_df[\"product_id\"].astype(str)\n",
    "\n",
    "# split data into train and test parition\n",
    "train_partition_final_row = int(0.8 * len(df))\n",
    "df_test = df[train_partition_final_row:]\n",
    "df = df[:train_partition_final_row].copy()\n",
    "\n",
    "df = pd.DataFrame(df[\"product_views\"].str.split(\",\").tolist(), index=df[\"id\"]).stack()\n",
    "df = df.reset_index([0, \"id\"])\n",
    "df.columns = [\"id\", \"product_id\"]\n",
    "df = df.groupby(df.columns.tolist(), as_index=False).size()\n",
    "df = df.reset_index(drop=False)\n",
    "df.columns = [\"id\", \"product_id\", \"clicks\"]\n",
    "df = df.dropna()\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "憋 Elapsed time for processing input data: 49.2 seconds\n"
     ]
    }
   ],
   "source": [
    "# Convert product_ids names into integer ids\n",
    "df[\"cookie_token\"] = df[\"id\"].astype(\"category\").cat.codes\n",
    "df[\"product_token\"] = df[\"product_id\"].astype(\"category\").cat.codes\n",
    "\n",
    "# Create a lookup frame so we can get the product_ids back later\n",
    "item_lookup = df[[\"product_token\", \"product_id\"]].drop_duplicates()\n",
    "item_lookup[\"product_token\"] = item_lookup.product_token.astype(str)\n",
    "df = df.drop([\"id\", \"product_id\"], axis=1)\n",
    "df = df.loc[df.clicks != 0]  # drop sessions without views (contain no information)\n",
    "df.head(3)\n",
    "\n",
    "# lists of all ids, product_ids and clicks\n",
    "ids = list(np.sort(df.cookie_token.unique()))\n",
    "product_ids = list(np.sort(df.product_token.unique()))\n",
    "clicks = list(df.clicks)\n",
    "\n",
    "# rows and columns for our new matrix\n",
    "rows = df.cookie_token.astype(float)\n",
    "cols = df.product_token.astype(float)\n",
    "\n",
    "# contruct a sparse matrix for our ids and items containing number of clicks\n",
    "data_sparse = sp.csr_matrix((clicks, (rows, cols)), shape=(len(ids), len(product_ids)))\n",
    "uids, iids = data_sparse.nonzero()\n",
    "\n",
    "print(\"憋 Elapsed time for processing input data: {:.3} seconds\".format(time.time() - t_prep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  DEFINE TENSORFLOW GRAPH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "\n",
    "def init_variable(size, dim, name=None):\n",
    "    \"\"\"\n",
    "    Helper function to initialize a new variable with\n",
    "    uniform random values.\n",
    "    \"\"\"\n",
    "    std = np.sqrt(2 / dim)\n",
    "    return tf.Variable(tf.random_uniform([size, dim], -std, std), name=name)\n",
    "\n",
    "\n",
    "def embed(inputs, size, dim, name=None):\n",
    "    \"\"\"\n",
    "    Helper function to get a Tensorflow variable and create\n",
    "    an embedding lookup to map our id and item\n",
    "    indices to vectors.\n",
    "    \"\"\"\n",
    "    emb = init_variable(size, dim, name)\n",
    "    return tf.nn.embedding_lookup(emb, inputs)\n",
    "\n",
    "\n",
    "def get_variable(graph, session, name):\n",
    "    \"\"\"\n",
    "    Helper function to get the value of a\n",
    "    Tensorflow variable by name.\n",
    "    \"\"\"\n",
    "    v = graph.get_operation_by_name(name)\n",
    "    v = v.values()[0]\n",
    "    v = v.eval(session=session)\n",
    "    return v\n",
    "\n",
    "\n",
    "with graph.as_default():\n",
    "    \"\"\"\n",
    "    Loss function:\n",
    "    -SUM ln (xui - xuj) + 位(w1)**2 + 位(w2)**2 + 位(w3)**2 ...\n",
    "    ln = the natural log\n",
    "    (xuij) = the sigmoid function of xuij.\n",
    "    位 = lambda regularization value.\n",
    "    ||W||**2 = the squared L2 norm of our model parameters.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Input into our model,  id (u), known item (i) an unknown item (i) triplets\n",
    "    u = tf.placeholder(tf.int32, shape=(None, 1))\n",
    "    i = tf.placeholder(tf.int32, shape=(None, 1))\n",
    "    j = tf.placeholder(tf.int32, shape=(None, 1))\n",
    "\n",
    "    # id feature embedding\n",
    "    u_factors = embed(u, len(ids), n_latent_vars, \"id_factors\")  # U matrix\n",
    "\n",
    "    # Known and unknown item embeddings\n",
    "    item_factors = init_variable(len(product_ids), n_latent_vars, \"item_factors\")  # V matrix\n",
    "    i_factors = tf.nn.embedding_lookup(item_factors, i)\n",
    "    j_factors = tf.nn.embedding_lookup(item_factors, j)\n",
    "\n",
    "    # i and j bias embeddings\n",
    "    item_bias = init_variable(len(product_ids), 1, \"item_bias\")\n",
    "    i_bias = tf.nn.embedding_lookup(item_bias, i)\n",
    "    i_bias = tf.reshape(i_bias, [-1, 1])\n",
    "    j_bias = tf.nn.embedding_lookup(item_bias, j)\n",
    "    j_bias = tf.reshape(j_bias, [-1, 1])\n",
    "\n",
    "    # Calculate the dot product + bias for known and unknown item to get xui and xuj\n",
    "    xui = i_bias + tf.reduce_sum(u_factors * i_factors, axis=2)\n",
    "    xuj = j_bias + tf.reduce_sum(u_factors * j_factors, axis=2)\n",
    "    xuij = xui - xuj\n",
    "\n",
    "    # Calculate the mean AUC (area under curve). If xuij is greater than 0, that means that xui is\n",
    "    # greater than xuj (and thats what we want).\n",
    "    u_auc = tf.reduce_mean(tf.cast(xuij > 0, float))\n",
    "    tf.summary.scalar(\"auc\", u_auc)\n",
    "\n",
    "    # Calculate the squared L2 norm ||W||**2 multiplied by 位\n",
    "    l2_norm = tf.add_n(\n",
    "        [\n",
    "            lambda_id * tf.reduce_sum(tf.multiply(u_factors, u_factors)),\n",
    "            lambda_item * tf.reduce_sum(tf.multiply(i_factors, i_factors)),\n",
    "            lambda_item * tf.reduce_sum(tf.multiply(j_factors, j_factors)),\n",
    "            lambda_bias * tf.reduce_sum(tf.multiply(i_bias, i_bias)),\n",
    "            lambda_bias * tf.reduce_sum(tf.multiply(j_bias, j_bias)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Calculate the loss as ||W||**2 - ln (Xuij)\n",
    "    # loss = l2_norm - tf.reduce_mean(tf.log(tf.sigmoid(xuij)))\n",
    "    loss = -tf.reduce_mean(tf.log(tf.sigmoid(xuij))) + l2_norm\n",
    "\n",
    "    # Train using the Adam optimizer to minimize our loss function\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    step = opt.minimize(loss)\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  MODEL TRAINING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf36b225a714a8ab77d75fcd12b75bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3072), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t_train = time.time()  # start timer for training\n",
    "\n",
    "session = tf.Session(config=None, graph=graph)\n",
    "session.run(init)\n",
    "\n",
    "progress = tqdm(total=batches * epochs)  # progress bar for the training.\n",
    "\n",
    "for _ in range(epochs):\n",
    "    for _ in range(batches):\n",
    "\n",
    "        # We want to sample one known and one unknown\n",
    "        # item for each id.\n",
    "\n",
    "        # First we sample 15000 uniform indices.\n",
    "        idx = np.random.randint(low=0, high=len(uids), size=n_triplets)\n",
    "\n",
    "        # We then grab the ids matching those indices\n",
    "        batch_u = uids[idx].reshape(-1, 1)\n",
    "\n",
    "        # Then the known items for those ids\n",
    "        batch_i = iids[idx].reshape(-1, 1)\n",
    "\n",
    "        # Lastly we randomly sample one unknown item for each id\n",
    "        batch_j = np.random.randint(\n",
    "            low=0, high=len(product_ids), size=(n_triplets, 1), dtype=\"int32\"\n",
    "        )\n",
    "\n",
    "        # Feed our ids, known and unknown items to\n",
    "        # our tensorflow graph.\n",
    "        feed_dict = {u: batch_u, i: batch_i, j: batch_j}\n",
    "\n",
    "        # We run the session.\n",
    "        _, l, auc = session.run([step, loss, u_auc], feed_dict)\n",
    "\n",
    "    progress.update(batches)\n",
    "    progress.set_description(\"Loss: %.3f | AUC: %.3f\" % (l, auc))\n",
    "\n",
    "progress.close()\n",
    "\n",
    "train_time = time.time() - t_train\n",
    "print(\n",
    "    \"憋 Elapsed time for training on {} sequences: {:.3} minutes\".format(len(df), train_time / 60)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  DEFINE SIMILARITY LOOK UP AND RECOMMENDATIONS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "def find_similar_product_ids(product_id=None, num_items=15):\n",
    "    \"\"\"Find product_ids similar to an product_id.\n",
    "    Args:\n",
    "        product_id (str): The name of the product_id we want to find similar product_ids for\n",
    "        num_items (int): How many similar product_ids we want to return.\n",
    "    Returns:\n",
    "        similar (pandas.DataFrame): DataFrame with num_items product_id names and scores\n",
    "    \"\"\"\n",
    "\n",
    "    id_vecs = get_variable(graph, session, \"id_factors\")  # matrix U\n",
    "    item_vecs = get_variable(graph, session, \"item_factors\")  # matrix V\n",
    "    item_bias = get_variable(graph, session, \"item_bias\").reshape(-1)\n",
    "    item_id = int(item_lookup[item_lookup.product_id == product_id][\"product_token\"])\n",
    "    item_vec = item_vecs[item_id].T  # Transpose item vector\n",
    "\n",
    "    # Calculate the similarity between this product and all other product_ids\n",
    "    # by multiplying the item vector with our item_matrix\n",
    "    scores = np.add(item_vecs.dot(item_vec), item_bias).reshape(1, -1)[0]\n",
    "    top_10 = np.argsort(scores)[::-1][:num_items]  # Indices of top similarities\n",
    "\n",
    "    # Map the indices to product_id names\n",
    "    product_ids, product_id_scores = [], []\n",
    "\n",
    "    for idx in top_10:\n",
    "        product_ids.append(\n",
    "            item_lookup.product_id.loc[item_lookup.product_token == str(idx)].iloc[0]\n",
    "        )\n",
    "        product_id_scores.append(scores[idx])\n",
    "\n",
    "    similar = pd.DataFrame({\"product_id\": product_ids, \"score\": product_id_scores})\n",
    "    similar[\"product_name\"] = similar[\"product_id\"].map(\n",
    "        dict(zip(product_map_df[\"product_id\"], product_map_df[\"product_name\"]))\n",
    "    )\n",
    "    similar[\"product_type_name\"] = similar[\"product_id\"].map(\n",
    "        dict(zip(product_map_df[\"product_id\"], product_map_df[\"product_type_name\"]))\n",
    "    )\n",
    "\n",
    "    return similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "def make_recommendation(cookie_token=None, num_items=5):\n",
    "    \"\"\"Recommend items for a given id given a trained model\n",
    "    Args:\n",
    "        cookie_token (int): The id of the id we want to create recommendations for.\n",
    "        num_items (int): How many recommendations we want to return.\n",
    "    Returns:\n",
    "        recommendations (pandas.DataFrame): DataFrame with num_items product_id names and scores\n",
    "    \"\"\"\n",
    "\n",
    "    # make df of the session for input token\n",
    "    clicks = df[df[\"cookie_token\"] == cookie_token].merge(\n",
    "        item_lookup, on=\"product_token\", how=\"left\"\n",
    "    )\n",
    "    clicks[\"product_name\"] = clicks[\"product_id\"].map(\n",
    "        dict(zip(product_map_df[\"product_id\"], product_map_df[\"product_name\"]))\n",
    "    )\n",
    "    clicks[\"product_type_name\"] = clicks[\"product_id\"].map(\n",
    "        dict(zip(product_map_df[\"product_id\"], product_map_df[\"product_type_name\"]))\n",
    "    )\n",
    "\n",
    "    print(\"Making implicit feedback recommendations for observed user views: \\n{}\".format(clicks))\n",
    "    print(\"\\n----------------------\\n\")\n",
    "\n",
    "    id_vecs = get_variable(graph, session, \"id_factors\")  # matrix U\n",
    "    item_vecs = get_variable(graph, session, \"item_factors\")  # matrix V\n",
    "    item_bias = get_variable(graph, session, \"item_bias\").reshape(-1)\n",
    "    rec_vector = np.add(id_vecs[cookie_token, :].dot(item_vecs.T), item_bias)\n",
    "    item_idx = np.argsort(rec_vector)[::-1][:num_items]  # get indices of top cooki\n",
    "\n",
    "    # Map the indices to product_id names\n",
    "    product_ids, scores = [], []\n",
    "\n",
    "    for idx in item_idx:\n",
    "        product_ids.append(\n",
    "            item_lookup.product_id.loc[item_lookup.product_token == str(idx)].iloc[0]\n",
    "        )\n",
    "        scores.append(rec_vector[idx])\n",
    "\n",
    "    recommendations = pd.DataFrame({\"product_id\": product_ids, \"score\": scores})\n",
    "    recommendations[\"product_name\"] = recommendations[\"product_id\"].map(\n",
    "        dict(zip(product_map_df[\"product_id\"], product_map_df[\"product_name\"]))\n",
    "    )\n",
    "    recommendations[\"product_type_name\"] = recommendations[\"product_id\"].map(\n",
    "        dict(zip(product_map_df[\"product_id\"], product_map_df[\"product_type_name\"]))\n",
    "    )\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  INVESTIGATE RESULTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "print(find_similar_product_ids(product_id=\"828805\"))  # Airpods 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "print(find_similar_product_ids(product_id=\"838335\"))  # iPhone 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "print(find_similar_product_ids(product_id=\"793672\"))  # iPhone 8 Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "print(find_similar_product_ids(product_id=\"817318\"))  # Sony noise-cancelling headphones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "print(find_similar_product_ids(product_id=\"834996\"))  # Macbook Pro 13\" touch bar 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "print(find_similar_product_ids(product_id=\"835003\"))  # Macbook Air 13\" 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "print(find_similar_product_ids(product_id=\"795117\"))  # Smart doorbell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "print(find_similar_product_ids(product_id=\"671720\"))  # Smart thermo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "print(find_similar_product_ids(product_id=\"812182\"))  # Kobo e-reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "print(find_similar_product_ids(product_id=\"828471\"))  # Dyson vacuum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "print(make_recommendation(cookie_token=3))  # Personal recommendations for user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "print(make_recommendation(cookie_token=4 ** 1))  # Personal recommendations for user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "print(make_recommendation(cookie_token=4 ** 5))  # Personal recommendations for user"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
